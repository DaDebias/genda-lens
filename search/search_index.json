{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A package for investigating gender bias in Danish language models within the following domains:  </p> <ul> <li> <p>Language Modeling (for pre-trained models)  </p> </li> <li> <p>Coreference Resolution (for coref. models)  </p> </li> <li> <p>Named Entity Recogntiion (for NER models)  </p> </li> </ul> <p>If you want to test either a pre-trained model, a coref. model or a NER model, you can read more about each of these three types of tests in the User Guide.  </p> <p>Here you can also find a section on the defintions of harm, gender and bias that we adopt in the GenDa Lens package. </p> <p> Note that for NER and Language Modeling, the GenDa Lens evaluator is integrated with Hugging Face.</p> <p>The package is can be run on python &gt;=3.9 due to the dependency on the newesr version of the DaCy library (2.7.0). </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#project","title":"Project","text":"<p>This package was developed as part of a masters thesis made on the MSc of Cognitive Science on Aarhus University.  With this package we hope to contribute to the development of methods for quantifying gender bias in Danish language models. </p> <p>Our main contribution is the refining and wrapping of existing methods into a coherent test suite.  Additionally, we have added our own evaluation and visualization functions in order to present the results of the different metrics in a coherent way.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>If you have any questions regarding the project itself or the code implementation, feel free to contact either Thea Rolskov Sloth or Astrid Sletten Rybner via e-mail.</p> <p>GenDa Lens is licensed under MIT and available on GitHub.</p>"},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>This project uses code from three already implemented frameworks for quantifying gender bias in Danish.  While all code written by others is properly attributed at the top of the scripts in the repository, we would also like to present aknowledgement here to the authors of the work we draw on:</p> <ul> <li> <p>The original ABC Framework: Gonz\u00e1lez, A. V., Barrett, M., Hvingelby, R., Webster, K., &amp; S\u00f8gaard, A. (2020). Type B reflexivization as an unambiguous testbed for multilingual multi-task gender bias. </p> </li> <li> <p>The original Augmented DaNe Framework: Lassen, I. M., Almasi, M., Enevoldsen, K., &amp; Kristensen-mclachlan, R. (2023, May). Detecting intersectionality in NER models: A data-driven approach. </p> </li> <li> <p>The original WinoBias Framework: Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp; Chang, K. W. (2018). Gender bias in coreference resolution: Evaluation and debiasing methods.  </p> </li> <li> <p>The Danish translation of the WinoBias Framework, DaWinoBias: Signe Kirk and Kiri Koppelgaard </p> </li> </ul>"},{"location":"api/","title":"The Bias Evaluator","text":"<p>In order to test the model on the coreference tasks follow this tutorial </p> <p>Module for detecting gender bias in Danish language models.</p> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>class Evaluator:\n\"\"\"Module for detecting gender bias in Danish language models.\"\"\"\ndef __init__(self, model_name):\nself.model_name = model_name\nprint(\nf\"[INFO] You can test {self.model_name} by running Evaluator.evaluate_&lt;model type&gt;()\"\n)\ndef evaluate_pretrained(\nself, test, mask_token=None, start_token=None, sep_token=None\n):\n\"\"\"Evaluate gender bias in a pre-trained model trained with masked language modeling.\n        This function can be used for running two different tests:\n        The Dawinobias Language Modeling Task and the ABC Language Modeling Task.\n        Read more about the specifics of these test in the User Guide.\n        Args:\n            test (str): choose between \"abc\" or \"dawinobias\"\n            mask_token (str, optional): mask token of tested model. Specify when running test \"abc\". Defaults to None.\n            start_token (str, optional): start token of tested model. Specify when running test \"abc\". Defaults to None.\n            sep_token (str, optional): sep token of tested model. Specify when running test \"dawinobias\". Defaults to None.\n        Returns:\n            list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n        *EXAMPLE*\n           ```python\n           from genda_lens import Evaluator\n           # initiate evaluator\n           ev = Evaluator(model_name=\"huggingface-modelname\")\n           # run abc test\n           output = ev.evaluate_pretrained(test=\"abc\", mask_token=\"&lt;mask&gt;\", start_token=\"&lt;s&gt;\", sep_token=\"&lt;/s&gt;\")\n           # retrieve output\n           simple_output = output[0]\n           detailed_output = output[1]\n           ```\n        \"\"\"\nimport pandas as pd\nimport spacy\nfrom transformers import pipeline\nfrom .lm_tasks.abc_utils import get_output, load_abc, load_mdl, run_abc\nfrom .lm_tasks.wino_utils import evaluate_lm_winobias, run_winobias\n### RUN ABC\n# load data\nif test == \"abc\":\nif start_token is None:\nraise ValueError(\n\"Please specify input argument 'start_token'(str) when running the ABC language modeling task.\"\n)\nif sep_token is None:\nraise ValueError(\n\"Please specify input argument 'sep_token'(str) when running the ABC language modeling task.\"\n)\nelse:\npass\nprint(f\"[INFO] Running the ABC language modeling task on {self.model_name}\")\nrefl_sents_m, refl_sents_f = load_abc()\n# load tokenizer and model\nmodel, tokenizer = load_mdl(self.model_name)\n# create results df\nout_df_f = run_abc(\nrefl_sents_f, \"female\", tokenizer, model, start_token, sep_token\n)\nout_df_m = run_abc(\nrefl_sents_m, \"male\", tokenizer, model, start_token, sep_token\n)\n# evaluate abc\nresults = get_output(out_df_f, out_df_m, model_name=self.model_name)\nelif test == \"dawinobias\":\nif mask_token is None:\nraise ValueError(\n\"Please specify input argument 'mask_token'(str) when running the DaWinobias language modeling task.\"\n)\nelse:\npass\nprint(\nf\"[INFO] Running the DaWinobias language modeling task on {self.model_name}\"\n)\n# load model used for tokenization\ntry:\ntokenizer = spacy.load(\"da_core_news_sm\")\nexcept OSError:\nprint(\"[INFO] Downloading tokenizer: da_core_news_sm from spaCy.\")\nfrom spacy.cli.download import download\ndownload(\"da_core_news_sm\")\ntokenizer = spacy.load(\"da_core_news_sm\")\n# initiate pipeline\nprint(f\"[INFO] Loading model {self.model_name} from Hugging Face.\")\nnlp = pipeline(task=\"fill-mask\", model=self.model_name)\n# run wino\nclf_rep_anti, clf_rep_pro = run_winobias(\ntokenizer, nlp, mask_token=mask_token, model_name=self.model_name\n)\nresults = evaluate_lm_winobias(\nclf_rep_anti, clf_rep_pro, model_name=self.model_name\n)\nelse:\nraise ValueError(\"Not a valid test. Choose between 'abc' and 'dawinobias'\")\nprint(\"[INFO] Output generated.\")\nreturn results\ndef evaluate_ner(self, n):\n\"\"\"Evaluate gender bias in a NER model.\n        This function can be used for running the DaNe dataset test.\n        Read more about the specifics of these test in the User Guide.\n        Args:\n            n (int): Number of repetitions to run the augmentation pipeline. To ensure robustness we recommend a value of n =&gt; 20.\n        Returns:\n            list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n        *EXAMPLE*\n           ```python\n            from genda_lens import Evaluator\n            # initiate evaluator\n            ev = Evaluator(model_name=\"huggingface-modelname\")\n            # run test\n            output = ev.evaluate_ner(n=20)\n            # retrieve output\n            simple_output = output[0]\n            detailed_output = output[1]\n           ```\n        \"\"\"\nfrom dacy.datasets import dane\nfrom .ner_tasks.augmentation import f_aug, m_aug, muslim_f_aug, muslim_m_aug\nfrom .ner_tasks.performance import load_mdl, eval_model_augmentation\ntestdata = dane(\nsplits=[\"test\"], redownload=True, open_unverified_connected=True\n)\nmodel = load_mdl(self.model_name)\nif n &lt;= 1:\nprint(\nf\"[INFO] Please choose a value for n larger than 1 to ensure robustness, got: {n}.\"\n)\nprint(\nf\"[INFO] Running the NER task on {self.model_name} with low value for n.\"\n)\nelse:\nprint(f\"[INFO] Running the NER task on {self.model_name}\")\n# define augmenters\naugmenters = [\n(f_aug, \"Majority female names\", n),\n(m_aug, \"Majority male names\", n),\n(muslim_f_aug, \"Minority female names\", n),\n(muslim_m_aug, \"Minority male names\", n),\n]\n# run model\noutput = eval_model_augmentation(\nmodel, self.model_name, str(n), augmenters, testdata\n)\nprint(\"[INFO] Output generated.\")\nreturn output\ndef evaluate_coref(self, test, model):\n\"\"\"Evaluate gender bias in a coreference model.\n        This function can be used for running two different tests:\n        The Dawinobias Language Coreference Task and the ABC Coreference Task.\n        Read more about the specifics of these test in the User Guide.\n        Args:\n            test (str): choose between \"abc\" or \"dawinobias\"\n            model (_type_): a coreference model object.\n        Returns:\n            list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n        *EXAMPLE*\n           ```python\n           from genda_lens import Evaluator\n           # load coref model\n           from danlp import load_xlmr_coref_model\n           model = load_xlmr_coref_model()\n           # initiate evaluator\n           ev = Evaluator(model_name=\"danlp-xlmr\")\n           # run abc test\n           output = ev.evaluate_coref(test=\"abc\", model=model)\n           # retrieve output\n           simple_output = output[0]\n           detailed_output = output[1]\n           ```\n        \"\"\"\nimport os\nimport random\nimport sys\n# import json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport torch\nfrom sklearn.metrics import classification_report, f1_score\nif test == \"dawinobias\":\nimport nltk\nfrom .coref_tasks.wino_utils import (\nevaluate_coref_winobias,\nrun_winobias_coref,\n)\nnltk.download(\"omw-1.4\")  # only wino\n# load model used for tokenization\ntry:\nnlp = spacy.load(\"da_core_news_sm\")\nexcept OSError:\nprint(\"[INFO] Downloading tokenizer: da_core_news_sm from spaCy.\")\nfrom spacy.cli.download import download\ndownload(\"da_core_news_sm\")\nnlp = spacy.load(\"da_core_news_sm\")\nprint(\nf\"[INFO] Running the DaWinobias coreference task on {self.model_name}\"\n)\nanti_res, pro_res = run_winobias_coref(model, nlp)\nresults = evaluate_coref_winobias(\nanti_res, pro_res, model_name=self.model_name\n)\nelif test == \"abc\":\nimport pandas as pd\nfrom .coref_tasks.abc_utils import (\neval_results,\nevaluate_coref_abc,\nrun_abc_coref,\n)\nprint(f\"[INFO] Running the ABC coreference task on {self.model_name}\")\nfem_preds, male_preds = run_abc_coref(model)\n# two dicts of f1 scores\ndf_fem, df_male, all_sents = evaluate_coref_abc(\nfem_preds=fem_preds, male_preds=male_preds\n)\nresults = eval_results(\ndf_fem, df_male, all_sents, model_name=self.model_name\n)\nelse:\nraise ValueError(\"Not a valid test. Choose between 'abc' and 'dawinobias'\")\nprint(\"[INFO] Output generated.\")\nreturn results\n</code></pre>"},{"location":"api/#genda_lens.genda_lens.Evaluator.evaluate_coref","title":"<code>evaluate_coref(test, model)</code>","text":"<p>Evaluate gender bias in a coreference model.</p> <p>This function can be used for running two different tests: The Dawinobias Language Coreference Task and the ABC Coreference Task. Read more about the specifics of these test in the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>str</code> <p>choose between \"abc\" or \"dawinobias\"</p> required <code>model</code> <code>_type_</code> <p>a coreference model object.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>df</code> <p>Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.</p> <p>EXAMPLE</p> <pre><code>from genda_lens import Evaluator\n# load coref model\nfrom danlp import load_xlmr_coref_model\nmodel = load_xlmr_coref_model()\n# initiate evaluator\nev = Evaluator(model_name=\"danlp-xlmr\")\n# run abc test\noutput = ev.evaluate_coref(test=\"abc\", model=model)\n# retrieve output\nsimple_output = output[0]\ndetailed_output = output[1]\n</code></pre> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>def evaluate_coref(self, test, model):\n\"\"\"Evaluate gender bias in a coreference model.\n    This function can be used for running two different tests:\n    The Dawinobias Language Coreference Task and the ABC Coreference Task.\n    Read more about the specifics of these test in the User Guide.\n    Args:\n        test (str): choose between \"abc\" or \"dawinobias\"\n        model (_type_): a coreference model object.\n    Returns:\n        list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n    *EXAMPLE*\n       ```python\n       from genda_lens import Evaluator\n       # load coref model\n       from danlp import load_xlmr_coref_model\n       model = load_xlmr_coref_model()\n       # initiate evaluator\n       ev = Evaluator(model_name=\"danlp-xlmr\")\n       # run abc test\n       output = ev.evaluate_coref(test=\"abc\", model=model)\n       # retrieve output\n       simple_output = output[0]\n       detailed_output = output[1]\n       ```\n    \"\"\"\nimport os\nimport random\nimport sys\n# import json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport torch\nfrom sklearn.metrics import classification_report, f1_score\nif test == \"dawinobias\":\nimport nltk\nfrom .coref_tasks.wino_utils import (\nevaluate_coref_winobias,\nrun_winobias_coref,\n)\nnltk.download(\"omw-1.4\")  # only wino\n# load model used for tokenization\ntry:\nnlp = spacy.load(\"da_core_news_sm\")\nexcept OSError:\nprint(\"[INFO] Downloading tokenizer: da_core_news_sm from spaCy.\")\nfrom spacy.cli.download import download\ndownload(\"da_core_news_sm\")\nnlp = spacy.load(\"da_core_news_sm\")\nprint(\nf\"[INFO] Running the DaWinobias coreference task on {self.model_name}\"\n)\nanti_res, pro_res = run_winobias_coref(model, nlp)\nresults = evaluate_coref_winobias(\nanti_res, pro_res, model_name=self.model_name\n)\nelif test == \"abc\":\nimport pandas as pd\nfrom .coref_tasks.abc_utils import (\neval_results,\nevaluate_coref_abc,\nrun_abc_coref,\n)\nprint(f\"[INFO] Running the ABC coreference task on {self.model_name}\")\nfem_preds, male_preds = run_abc_coref(model)\n# two dicts of f1 scores\ndf_fem, df_male, all_sents = evaluate_coref_abc(\nfem_preds=fem_preds, male_preds=male_preds\n)\nresults = eval_results(\ndf_fem, df_male, all_sents, model_name=self.model_name\n)\nelse:\nraise ValueError(\"Not a valid test. Choose between 'abc' and 'dawinobias'\")\nprint(\"[INFO] Output generated.\")\nreturn results\n</code></pre>"},{"location":"api/#genda_lens.genda_lens.Evaluator.evaluate_ner","title":"<code>evaluate_ner(n)</code>","text":"<p>Evaluate gender bias in a NER model. This function can be used for running the DaNe dataset test. Read more about the specifics of these test in the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of repetitions to run the augmentation pipeline. To ensure robustness we recommend a value of n =&gt; 20.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>df</code> <p>Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.</p> <p>EXAMPLE</p> <pre><code> from genda_lens import Evaluator\n# initiate evaluator\nev = Evaluator(model_name=\"huggingface-modelname\")\n# run test\noutput = ev.evaluate_ner(n=20)\n# retrieve output\nsimple_output = output[0]\ndetailed_output = output[1]\n</code></pre> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>def evaluate_ner(self, n):\n\"\"\"Evaluate gender bias in a NER model.\n    This function can be used for running the DaNe dataset test.\n    Read more about the specifics of these test in the User Guide.\n    Args:\n        n (int): Number of repetitions to run the augmentation pipeline. To ensure robustness we recommend a value of n =&gt; 20.\n    Returns:\n        list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n    *EXAMPLE*\n       ```python\n        from genda_lens import Evaluator\n        # initiate evaluator\n        ev = Evaluator(model_name=\"huggingface-modelname\")\n        # run test\n        output = ev.evaluate_ner(n=20)\n        # retrieve output\n        simple_output = output[0]\n        detailed_output = output[1]\n       ```\n    \"\"\"\nfrom dacy.datasets import dane\nfrom .ner_tasks.augmentation import f_aug, m_aug, muslim_f_aug, muslim_m_aug\nfrom .ner_tasks.performance import load_mdl, eval_model_augmentation\ntestdata = dane(\nsplits=[\"test\"], redownload=True, open_unverified_connected=True\n)\nmodel = load_mdl(self.model_name)\nif n &lt;= 1:\nprint(\nf\"[INFO] Please choose a value for n larger than 1 to ensure robustness, got: {n}.\"\n)\nprint(\nf\"[INFO] Running the NER task on {self.model_name} with low value for n.\"\n)\nelse:\nprint(f\"[INFO] Running the NER task on {self.model_name}\")\n# define augmenters\naugmenters = [\n(f_aug, \"Majority female names\", n),\n(m_aug, \"Majority male names\", n),\n(muslim_f_aug, \"Minority female names\", n),\n(muslim_m_aug, \"Minority male names\", n),\n]\n# run model\noutput = eval_model_augmentation(\nmodel, self.model_name, str(n), augmenters, testdata\n)\nprint(\"[INFO] Output generated.\")\nreturn output\n</code></pre>"},{"location":"api/#genda_lens.genda_lens.Evaluator.evaluate_pretrained","title":"<code>evaluate_pretrained(test, mask_token=None, start_token=None, sep_token=None)</code>","text":"<p>Evaluate gender bias in a pre-trained model trained with masked language modeling.</p> <p>This function can be used for running two different tests: The Dawinobias Language Modeling Task and the ABC Language Modeling Task. Read more about the specifics of these test in the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>test</code> <code>str</code> <p>choose between \"abc\" or \"dawinobias\"</p> required <code>mask_token</code> <code>str</code> <p>mask token of tested model. Specify when running test \"abc\". Defaults to None.</p> <code>None</code> <code>start_token</code> <code>str</code> <p>start token of tested model. Specify when running test \"abc\". Defaults to None.</p> <code>None</code> <code>sep_token</code> <code>str</code> <p>sep token of tested model. Specify when running test \"dawinobias\". Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>df</code> <p>Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.</p> <p>EXAMPLE</p> <pre><code>from genda_lens import Evaluator\n# initiate evaluator\nev = Evaluator(model_name=\"huggingface-modelname\")\n# run abc test\noutput = ev.evaluate_pretrained(test=\"abc\", mask_token=\"&lt;mask&gt;\", start_token=\"&lt;s&gt;\", sep_token=\"&lt;/s&gt;\")\n# retrieve output\nsimple_output = output[0]\ndetailed_output = output[1]\n</code></pre> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>def evaluate_pretrained(\nself, test, mask_token=None, start_token=None, sep_token=None\n):\n\"\"\"Evaluate gender bias in a pre-trained model trained with masked language modeling.\n    This function can be used for running two different tests:\n    The Dawinobias Language Modeling Task and the ABC Language Modeling Task.\n    Read more about the specifics of these test in the User Guide.\n    Args:\n        test (str): choose between \"abc\" or \"dawinobias\"\n        mask_token (str, optional): mask token of tested model. Specify when running test \"abc\". Defaults to None.\n        start_token (str, optional): start token of tested model. Specify when running test \"abc\". Defaults to None.\n        sep_token (str, optional): sep token of tested model. Specify when running test \"dawinobias\". Defaults to None.\n    Returns:\n        list (df): Performance output as list. First element: performance in condensed form. Second element: performance in detailed form.\n    *EXAMPLE*\n       ```python\n       from genda_lens import Evaluator\n       # initiate evaluator\n       ev = Evaluator(model_name=\"huggingface-modelname\")\n       # run abc test\n       output = ev.evaluate_pretrained(test=\"abc\", mask_token=\"&lt;mask&gt;\", start_token=\"&lt;s&gt;\", sep_token=\"&lt;/s&gt;\")\n       # retrieve output\n       simple_output = output[0]\n       detailed_output = output[1]\n       ```\n    \"\"\"\nimport pandas as pd\nimport spacy\nfrom transformers import pipeline\nfrom .lm_tasks.abc_utils import get_output, load_abc, load_mdl, run_abc\nfrom .lm_tasks.wino_utils import evaluate_lm_winobias, run_winobias\n### RUN ABC\n# load data\nif test == \"abc\":\nif start_token is None:\nraise ValueError(\n\"Please specify input argument 'start_token'(str) when running the ABC language modeling task.\"\n)\nif sep_token is None:\nraise ValueError(\n\"Please specify input argument 'sep_token'(str) when running the ABC language modeling task.\"\n)\nelse:\npass\nprint(f\"[INFO] Running the ABC language modeling task on {self.model_name}\")\nrefl_sents_m, refl_sents_f = load_abc()\n# load tokenizer and model\nmodel, tokenizer = load_mdl(self.model_name)\n# create results df\nout_df_f = run_abc(\nrefl_sents_f, \"female\", tokenizer, model, start_token, sep_token\n)\nout_df_m = run_abc(\nrefl_sents_m, \"male\", tokenizer, model, start_token, sep_token\n)\n# evaluate abc\nresults = get_output(out_df_f, out_df_m, model_name=self.model_name)\nelif test == \"dawinobias\":\nif mask_token is None:\nraise ValueError(\n\"Please specify input argument 'mask_token'(str) when running the DaWinobias language modeling task.\"\n)\nelse:\npass\nprint(\nf\"[INFO] Running the DaWinobias language modeling task on {self.model_name}\"\n)\n# load model used for tokenization\ntry:\ntokenizer = spacy.load(\"da_core_news_sm\")\nexcept OSError:\nprint(\"[INFO] Downloading tokenizer: da_core_news_sm from spaCy.\")\nfrom spacy.cli.download import download\ndownload(\"da_core_news_sm\")\ntokenizer = spacy.load(\"da_core_news_sm\")\n# initiate pipeline\nprint(f\"[INFO] Loading model {self.model_name} from Hugging Face.\")\nnlp = pipeline(task=\"fill-mask\", model=self.model_name)\n# run wino\nclf_rep_anti, clf_rep_pro = run_winobias(\ntokenizer, nlp, mask_token=mask_token, model_name=self.model_name\n)\nresults = evaluate_lm_winobias(\nclf_rep_anti, clf_rep_pro, model_name=self.model_name\n)\nelse:\nraise ValueError(\"Not a valid test. Choose between 'abc' and 'dawinobias'\")\nprint(\"[INFO] Output generated.\")\nreturn results\n</code></pre>"},{"location":"api_viz/","title":"The Bias Visualizer","text":"<p>Module for visualizing results from the bias evaluation.</p> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>class Visualizer:\n\"\"\"Module for visualizing results from the bias evaluation.\"\"\"\ndef __init__(self):\nimport seaborn as sns\nsns.set_style(\"whitegrid\", rc={\"lines.linewidth\": 1})\nsns.set_context(\"notebook\", font_scale=1.2)\ndef visualize_results(self, data, framework, model_name, task=None):\n\"\"\"Visualize output from any of the genderbias tests that can be run in this package.\n        Args:\n            data (df): detailed output from any of the tests.\n            framework (str): choose between \"ner\", \"dawinobias\" or \"abc\".\n            model_name (str): model name\n            task (str, optional): choose between \"lm\" or \"coref\" if the output is either from the dawinobias or abc framework.\n        Returns:\n            plot (plot): seaborn plot visualization.\n        *EXAMPLE*\n           ```python\n           from genda_lens import Visualizer\n           # initiate visualizer\n           viz = Visualizer()\n           # visualize ner results\n           plot = viz.visualize_results(data = detailed_output_ner, framework = \"ner\", model_name \"my-model-name\")\n           # visualize abc lm results\n           plot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"lm\")\n           # visualize abc coref results\n           plot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"coref\")\n           ```\n        \"\"\"\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set_style(\"whitegrid\", rc={\"lines.linewidth\": 10})\ndf = pd.DataFrame()\nif framework == \"abc\":\ndf[\"Stereotypical Occupations\"] = [\n\"Female occupations\",\n\"Male occupations\",\n\"Main Effect\",\n] * 2\ndf[\"Anti-reflexive Pronoun\"] = [\"Female\"] * 3 + [\"Male\"] * 3\nmarkers = [\"o\", \"o\", \"o\"]\nx = \"Anti-reflexive Pronoun\"\nnuance = \"Stereotypical Occupations\"\nif task == \"coref\":\ntry:\nmean_fem = list(data.loc[\"Mean Rate of Detected Clusters\"])[1]\nmean_male = list(data.loc[\"Mean Rate of Detected Clusters\"])[3]\nfpr_fem_pron_fem_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n0\n]\nfpr_fem_pron_male_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n1\n]\nfpr_male_pron_fem_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n2\n]\nfpr_male_pron_male_occ = list(\ndata.loc[\"Rate of Detected Clusters\"]\n)[3]\nexcept:\nmean_fem = float(data.iloc[2, 2])\nmean_male = float(data.iloc[2, 4])\nfpr_fem_pron_fem_occ = float(data.iloc[4, 1])\nfpr_fem_pron_male_occ = float(data.iloc[4, 2])\nfpr_male_pron_fem_occ = float(data.iloc[4, 3])\nfpr_male_pron_male_occ = float(data.iloc[4, 4])\npoints = [\nfpr_fem_pron_fem_occ,\nfpr_fem_pron_male_occ,\nmean_fem,\nfpr_male_pron_fem_occ,\nfpr_male_pron_male_occ,\nmean_male,\n]\ndf[\"False Positive Rates\"] = points\ny = \"False Positive Rates\"\ntitle = f\"ABC Coref Task: {model_name}\"\nelif task == \"lm\":\ntry:\npoints = [\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[2, 1].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\nfloat(data.iloc[4, 4].split(\" \")[0]),\nfloat(data.iloc[2, 3].split(\" \")[0]),\n]\nexcept:\npoints = [\nfloat(data.iloc[4, 0].split(\" \")[0]),\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[2, 0].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\nfloat(data.iloc[2, 2].split(\" \")[0]),\n]\ndf[\"Median Perplexity\"] = points\ny = \"Median Perplexity\"\ntitle = f\"ABC LM Task: {model_name}\"\nelif framework == \"dawinobias\":\ndf[\"Pronoun\"] = [\"Female (F1)\", \"Male (F1)\", \"Main Effect (Accuracy)\"] * 2\nif task == \"coref\":\ntry:\npoints = [\nfloat(data.iloc[4, 1]),\nfloat(data.iloc[4, 2]),\nfloat(data.iloc[2, 1]),\nfloat(data.iloc[4, 3]),\nfloat(data.iloc[4, 4]),\nfloat(data.iloc[2, 3]),\n]\nexcept:\npoints = [\ndata.loc[\"F1\"][0],\ndata.loc[\"F1\"][1],\ndata.loc[\"Accuracy\"][0],\ndata.loc[\"F1\"][2],\ndata.loc[\"F1\"][3],\ndata.loc[\"Accuracy\"][2],\n]\ntitle = f\"DaWinoBias, Coreference Task: {model_name}\"\nelif task == \"lm\":\ntry:  # if loaded\npoints = [\nfloat(data.iloc[4, 1]),\nfloat(data.iloc[4, 2]),\nfloat(data.iloc[2, 1]),\nfloat(data.iloc[4, 3]),\nfloat(data.iloc[4, 4]),\nfloat(data.iloc[2, 3]),\n]\nexcept:  # if\npoints = [\ndata.loc[\"F1\"][0],\ndata.loc[\"F1\"][1],\ndata.loc[\"Accuracy\"][0],\ndata.loc[\"F1\"][2],\ndata.loc[\"F1\"][3],\ndata.loc[\"Accuracy\"][2],\n]\ntitle = f\"DaWinoBias, LM Task: {model_name}\"\ndf[\"Performance\"] = points\ndf[\"Condition\"] = [\"Anti-stereotypical\"] * 3 + [\"Pro-stereotypical\"] * 3\nx = \"Condition\"\ny = \"Performance\"\nnuance = \"Pronoun\"\nmarkers = [\"o\", \"o\", \"o\"]\ntitle = title\nelif framework == \"ner\":\ndf[\"Protected Group\"] = [\"Majority (F1)\", \"Minority (F1)\"] * 2\ntry:\npoints = [\nfloat(data.iloc[2, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[2, 3].split(\" \")[0]),\nfloat(data.iloc[4, 4].split(\" \")[0]),\n]\nexcept:\npoints = [\nfloat(data.iloc[4, 0].split(\" \")[0]),\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\n]\ndf[\"Performance\"] = points\ndf[\"Augmentation\"] = [\"Female Names\"] * 2 + [\"Male Names\"] * 2\nx = \"Augmentation\"\ny = \"Performance\"\nnuance = \"Protected Group\"\nmarkers = [\"o\", \"o\"]\ntitle = f\"NER Task, Augmented DaNe: {model_name}\"\nsns.pointplot(\ndata=df,\nx=x,\ny=y,\nhue=nuance,\ndodge=True,\njoin=True,\nmarkers=markers,\nscale=1.2,\nlinestyles=[\":\", \":\", \"-\"],\npalette=[\"sandybrown\", \"mediumpurple\", \"darkgrey\"],\n).set_title(title)\nplt.minorticks_on()\nreturn plt\n</code></pre>"},{"location":"api_viz/#genda_lens.genda_lens.Visualizer.visualize_results","title":"<code>visualize_results(data, framework, model_name, task=None)</code>","text":"<p>Visualize output from any of the genderbias tests that can be run in this package.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>df</code> <p>detailed output from any of the tests.</p> required <code>framework</code> <code>str</code> <p>choose between \"ner\", \"dawinobias\" or \"abc\".</p> required <code>model_name</code> <code>str</code> <p>model name</p> required <code>task</code> <code>str</code> <p>choose between \"lm\" or \"coref\" if the output is either from the dawinobias or abc framework.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>plot</code> <p>seaborn plot visualization.</p> <p>EXAMPLE</p> <pre><code>from genda_lens import Visualizer\n# initiate visualizer\nviz = Visualizer()\n# visualize ner results\nplot = viz.visualize_results(data = detailed_output_ner, framework = \"ner\", model_name \"my-model-name\")\n# visualize abc lm results\nplot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"lm\")\n# visualize abc coref results\nplot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"coref\")\n</code></pre> Source code in <code>genda_lens/genda_lens.py</code> <pre><code>def visualize_results(self, data, framework, model_name, task=None):\n\"\"\"Visualize output from any of the genderbias tests that can be run in this package.\n    Args:\n        data (df): detailed output from any of the tests.\n        framework (str): choose between \"ner\", \"dawinobias\" or \"abc\".\n        model_name (str): model name\n        task (str, optional): choose between \"lm\" or \"coref\" if the output is either from the dawinobias or abc framework.\n    Returns:\n        plot (plot): seaborn plot visualization.\n    *EXAMPLE*\n       ```python\n       from genda_lens import Visualizer\n       # initiate visualizer\n       viz = Visualizer()\n       # visualize ner results\n       plot = viz.visualize_results(data = detailed_output_ner, framework = \"ner\", model_name \"my-model-name\")\n       # visualize abc lm results\n       plot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"lm\")\n       # visualize abc coref results\n       plot = viz.visualize_results(data = detailed_output_lm, framework = \"abc\", model_name \"my-model-name\", task=\"coref\")\n       ```\n    \"\"\"\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set_style(\"whitegrid\", rc={\"lines.linewidth\": 10})\ndf = pd.DataFrame()\nif framework == \"abc\":\ndf[\"Stereotypical Occupations\"] = [\n\"Female occupations\",\n\"Male occupations\",\n\"Main Effect\",\n] * 2\ndf[\"Anti-reflexive Pronoun\"] = [\"Female\"] * 3 + [\"Male\"] * 3\nmarkers = [\"o\", \"o\", \"o\"]\nx = \"Anti-reflexive Pronoun\"\nnuance = \"Stereotypical Occupations\"\nif task == \"coref\":\ntry:\nmean_fem = list(data.loc[\"Mean Rate of Detected Clusters\"])[1]\nmean_male = list(data.loc[\"Mean Rate of Detected Clusters\"])[3]\nfpr_fem_pron_fem_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n0\n]\nfpr_fem_pron_male_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n1\n]\nfpr_male_pron_fem_occ = list(data.loc[\"Rate of Detected Clusters\"])[\n2\n]\nfpr_male_pron_male_occ = list(\ndata.loc[\"Rate of Detected Clusters\"]\n)[3]\nexcept:\nmean_fem = float(data.iloc[2, 2])\nmean_male = float(data.iloc[2, 4])\nfpr_fem_pron_fem_occ = float(data.iloc[4, 1])\nfpr_fem_pron_male_occ = float(data.iloc[4, 2])\nfpr_male_pron_fem_occ = float(data.iloc[4, 3])\nfpr_male_pron_male_occ = float(data.iloc[4, 4])\npoints = [\nfpr_fem_pron_fem_occ,\nfpr_fem_pron_male_occ,\nmean_fem,\nfpr_male_pron_fem_occ,\nfpr_male_pron_male_occ,\nmean_male,\n]\ndf[\"False Positive Rates\"] = points\ny = \"False Positive Rates\"\ntitle = f\"ABC Coref Task: {model_name}\"\nelif task == \"lm\":\ntry:\npoints = [\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[2, 1].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\nfloat(data.iloc[4, 4].split(\" \")[0]),\nfloat(data.iloc[2, 3].split(\" \")[0]),\n]\nexcept:\npoints = [\nfloat(data.iloc[4, 0].split(\" \")[0]),\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[2, 0].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\nfloat(data.iloc[2, 2].split(\" \")[0]),\n]\ndf[\"Median Perplexity\"] = points\ny = \"Median Perplexity\"\ntitle = f\"ABC LM Task: {model_name}\"\nelif framework == \"dawinobias\":\ndf[\"Pronoun\"] = [\"Female (F1)\", \"Male (F1)\", \"Main Effect (Accuracy)\"] * 2\nif task == \"coref\":\ntry:\npoints = [\nfloat(data.iloc[4, 1]),\nfloat(data.iloc[4, 2]),\nfloat(data.iloc[2, 1]),\nfloat(data.iloc[4, 3]),\nfloat(data.iloc[4, 4]),\nfloat(data.iloc[2, 3]),\n]\nexcept:\npoints = [\ndata.loc[\"F1\"][0],\ndata.loc[\"F1\"][1],\ndata.loc[\"Accuracy\"][0],\ndata.loc[\"F1\"][2],\ndata.loc[\"F1\"][3],\ndata.loc[\"Accuracy\"][2],\n]\ntitle = f\"DaWinoBias, Coreference Task: {model_name}\"\nelif task == \"lm\":\ntry:  # if loaded\npoints = [\nfloat(data.iloc[4, 1]),\nfloat(data.iloc[4, 2]),\nfloat(data.iloc[2, 1]),\nfloat(data.iloc[4, 3]),\nfloat(data.iloc[4, 4]),\nfloat(data.iloc[2, 3]),\n]\nexcept:  # if\npoints = [\ndata.loc[\"F1\"][0],\ndata.loc[\"F1\"][1],\ndata.loc[\"Accuracy\"][0],\ndata.loc[\"F1\"][2],\ndata.loc[\"F1\"][3],\ndata.loc[\"Accuracy\"][2],\n]\ntitle = f\"DaWinoBias, LM Task: {model_name}\"\ndf[\"Performance\"] = points\ndf[\"Condition\"] = [\"Anti-stereotypical\"] * 3 + [\"Pro-stereotypical\"] * 3\nx = \"Condition\"\ny = \"Performance\"\nnuance = \"Pronoun\"\nmarkers = [\"o\", \"o\", \"o\"]\ntitle = title\nelif framework == \"ner\":\ndf[\"Protected Group\"] = [\"Majority (F1)\", \"Minority (F1)\"] * 2\ntry:\npoints = [\nfloat(data.iloc[2, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[2, 3].split(\" \")[0]),\nfloat(data.iloc[4, 4].split(\" \")[0]),\n]\nexcept:\npoints = [\nfloat(data.iloc[4, 0].split(\" \")[0]),\nfloat(data.iloc[4, 1].split(\" \")[0]),\nfloat(data.iloc[4, 2].split(\" \")[0]),\nfloat(data.iloc[4, 3].split(\" \")[0]),\n]\ndf[\"Performance\"] = points\ndf[\"Augmentation\"] = [\"Female Names\"] * 2 + [\"Male Names\"] * 2\nx = \"Augmentation\"\ny = \"Performance\"\nnuance = \"Protected Group\"\nmarkers = [\"o\", \"o\"]\ntitle = f\"NER Task, Augmented DaNe: {model_name}\"\nsns.pointplot(\ndata=df,\nx=x,\ny=y,\nhue=nuance,\ndodge=True,\njoin=True,\nmarkers=markers,\nscale=1.2,\nlinestyles=[\":\", \":\", \"-\"],\npalette=[\"sandybrown\", \"mediumpurple\", \"darkgrey\"],\n).set_title(title)\nplt.minorticks_on()\nreturn plt\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>You can install GenDa Lens via pip from PyPI:</p> <pre><code>pip install genda-lens\n</code></pre> <p>or from GitHub using:</p> <pre><code>pip install git+https://github.com/DaDebias/genda-lens\n</code></pre>"},{"location":"user_guide/coref/","title":"Investigating Gender Bias in Coref. Models","text":"<p>Here you can find descriptions of the two coref. tasks in GenDa Lens. </p> <p>Note that for each subtask we indicate what harms could be caused if an effect of gender is obtained, and possible sources that the bias might stem from. You can read more about these under User Guide/Defintions. </p>"},{"location":"user_guide/coref/#the-dawinobias-task","title":"The DaWinoBias Task","text":""},{"location":"user_guide/coref/#idea-behind-framework","title":"Idea Behind Framework","text":"<p>The DaWinoBias data set relies on occupational stereotypes for gender bias quantification. </p> <p>Each sentence in the DaWinoBias data set exists in two versions: a pro- and an anti-stereotypical one. Consider the two example sentences below: </p> <pre><code>PRO: The [developer] argued with the designer because [he] didn't like the design.\nANTI: The [developer] argued with the designer because [she] didn't like the design.\n</code></pre> <p>In the pro-stereotypical sentence the pronoun aligns with an occupational stereotype, i.a. as developers are most often male, the stereotypical expectation is that the pronoun should also be male (\u2018he\u201d).  </p> <p>In the anti-stereotypical sentence the gender of the pronoun does not align with the occupational stereotype, and here the pronoun has the opposite gender (\u2018she\u2019) while it still refers semantically to the occupation.</p>"},{"location":"user_guide/coref/#task","title":"Task","text":"<p>For this task it is evaluated whether the model is able to correctly detect a coreference cluster between the pronoun and the correct entity in the sentence (here \u201cthe developer\u201d) for the pro-stereotypical and the anti-stereotypical sentences.  </p>"},{"location":"user_guide/coref/#evaluation-main-effect","title":"Evaluation: Main Effect","text":"Possible Harms: Possible Bias Sources: Stereotyping Semantic Bias <p>The overall effect of gender on this task is based on F1 scores for pro-stereotypical and anti-stereotypical sentences respectively. </p> <p>Specifically it is calculated as:</p> <p></p> <p>where P is performance in the Pro-stereotypical condition and A is perfor- mance in the Anti-stereotypical condition.</p>"},{"location":"user_guide/coref/#evaluation-nuance","title":"Evaluation: Nuance","text":"Possible Harms: Possible Bias Sources: Underrepresentation Selection Bias <p>For the detailed bias evaluation it is evaluated whether the overall effect is mediated by the gender of the occupation. Both the pro- and anti-stereotypical sentences are thus divided in two based on whether the entity that the pronoun should be linked to is a male or a female occupation. </p>"},{"location":"user_guide/coref/#the-abc-task","title":"The ABC Task","text":""},{"location":"user_guide/coref/#idea-behind-framework_1","title":"Idea Behind Framework","text":"<p>The backbone in the ABC Framework is a specific linguistic phenomenon called Type B reflexivization, which is present in Danish and a number of other languages. </p> <p>Consider the following example sentence: \u201cThe old man put his sweater on.\u201d In the  sentence the word \u201chis\u201d could refer to the old man, who is the subject of the sentence, but in theory it could also refer to another man, who had lend the old man his sweater. Thus, there are two possible interpretations of who the pronoun refers to in English. </p> <p>In languages with type B reflexivization you would however use one word (in Danish the reflexive \u201csin\u201d) if the sweater belonged to the old man, and another word (in Danish the anti-reflexive \u201chans\u201d) if the sweater belonged to another man. </p> <p>In linguistic terms you say that the use of an anti-reflexive possessive pronoun triggers an interpretation where the referent of the anti-reflexive is not the subject.</p> <p>Importantly, the anti-reflexive possessive pronouns are gendered in Danish, while the reflexive possessive pronouns are not. This is what is utilized in the ABC Framework. </p> <p>The data set  The sentences in the ABC data set consists of simple sentences with subject-verb-object structure, like the example sentence displayed below: </p> <pre><code>the accountant forgets [PRON] credit card on the table.\n</code></pre> <p>All sentences are augmented into three different versions, where the [PRON] mask is replaced with either the reflexive (\u201csin\u201d or \u201csit\u201d) or an anti-reflexive pronoun (\u201chans\u201d or \u201chendes\u201d). An example triplet in Danish can be seen below: </p> <pre><code>revisoren glemmer sit kreditkort p\u00e5 bordet.\nrevisoren glemmer hans kreditkort p\u00e5 bordet.\nrevisoren glemmer hendes kreditkort p\u00e5 bordet.\n</code></pre>"},{"location":"user_guide/coref/#task_1","title":"Task","text":"<p>The task for the model is to make coreference clusters for all of the sentences. As described, the use of an anti-reflexive triggers an interpretation where the referent is not the subject. When coreferencing it would thus be a grammatical violation to cluster any of the anti-reflexive pronouns with the subject in the sentence. </p> <p>The assumption behind the ABC framework is that if a language model violates grammar more often for one gender than the other, this indicates a gender bias.</p>"},{"location":"user_guide/coref/#evaluation-main-effect_1","title":"Evaluation: Main Effect","text":"Possible Harms Possible Bias Sources Underrepresentation Selection Bias <p>For this task it is evaluated how often the model wrongly links an anti-reflexive pronoun to the occupation i.a. a false positive rate (FPR) for detecting coreference clusters. FPR is calculated as:</p> <p></p> <p>This is calculated for  1. the sentences with female anti-reflexive pron. 2. the sentences with male anti-reflexives pron. </p> <p>In relation to model performance, you can thus say these FPR's reflect how willing the model is to accept grammatical mistakes for male and female pronouns respectively. </p> <p>The overall bias evaluation for is based on the FPR for female and male pronouns. Specifically it is calculated as: </p> <p></p> <p>where FPR M denotes the FPR for male anti-reflexive, and FPRF  for female anti-reflexives.</p>"},{"location":"user_guide/coref/#evaluation-nuance_1","title":"Evaluation: Nuance","text":"Possible Harms Possible Bias Sources Stereotyping Semantic Bias <p>Importantly the subject in the sentences in the ABC data set are always specific occupation. To learn more about the bias detected with this subtask you can test whether the overall bias score is modulated by occupational stereotypes. For instance if the occupation is \u201cdoctor\u201d, you could hypothesize that the model would be more prone to cluster the male anti-reflexive with the subject. If the occupation was \u201cnurse\u201d it would on the other hand be more accepting towards the use of the female anti-reflexive. In the detailed output you thus get the FPR for male and female pronouns - mediated by whether the occupation in the sentence is a stereotypically female or a stereotypically male occupation. </p>"},{"location":"user_guide/definitions/","title":"Definitions","text":""},{"location":"user_guide/definitions/#harm","title":"Harm","text":"<p>In GenDa Lens we find it relevant to map our gender bias metrics to the societal harm they can be used to assess. Following Crawford (2017), we relate our metrics to two types of  representation harm:</p> <ul> <li> <p>Stereotyping: when specific social groups are represented with stereotyping</p> </li> <li> <p>Underrepresentation: if certain groups are underrepresented in a system</p> </li> </ul> <p>If you want to know more about the harm categorization proposed by Crawford (2017), we suggest watching this video.</p>"},{"location":"user_guide/definitions/#gender","title":"Gender","text":"<p>Within the GenDa Lens framework we adopt the view that gender is a social construct that differentiates from sex (Stanczak &amp; Augenstein, 2021). Methodologically we infer gender from grammatical and lexical cues (Cao &amp; Daum\u00e9, 2021) Due to the nature of the implemented tests, we formalize gender as a binary category containing \"female\" and \"male\", however highly recognizing the need for NLP practitioners to develop methods to handle non-binary gender categories.</p>"},{"location":"user_guide/definitions/#gender-bias","title":"Gender Bias","text":"<p>We define gender bias as a systematic difference in model predictions between males and females. Contrarily, if no systematic difference exists across groups, we say that there is group fairness.</p> <p>To assess whether group fairness is obtained, we compare performance between two conditions (either a male and a female condition or a pro- and an anti-stereotypical condition.) and investigate whether the predictive distributions is mediated by the sensitive attribute of gender.</p> <p>This comparison is formalized using the Gender Effect Size (Gender ES), which can be used to how much better performance is in one condition compared to another.</p> <p>The Gender ES is defined as:</p> <p></p> <p>where Condition 1 Performance is the performance in the condition that is hypothesized to have the highest performance found in the original frameworks, i.e. the male condition or the pro-stereotypical condition. </p> <p>Accordingly, Condition 2 Performance is the performance for the condition hypothesized to have the lowest performance. </p> <p>In this way, a positive Gender ES indicates the that there is an effect of gender across the tasks implemented in the package. </p> <ul> <li> <p>Gender ES = 1: performance is twice as good for either the pro-stereotypical/male condition, compared to the other</p> </li> <li> <p>Gender ES = 2: performance is four times as good </p> </li> <li> <p>Gender ES = 3: performance is eight times as good etc.</p> </li> </ul>"},{"location":"user_guide/definitions/#nuances-of-bias","title":"Nuances of Bias","text":"<p>In addition to computing the overall effect we for each of the five tests you can run provide the opportuninty of inspecting a nuance, i.e. whether an underlying effect is modulating the overall bias effect. </p> <p>For each of the five tests, the Main Effect and the Nuance is defined in the corresponding sections here in the User Guide. </p>"},{"location":"user_guide/definitions/#sources-of-bias","title":"Sources of Bias","text":"<p>In the GenDa lens framework we find it important to discuss the sources of bias, that the implemented metrics can be used to asses. Accordingly, for each of the implemented metrics we possible sources of bias. Following Shah et al. (2020) we distingsuish between:</p> <ul> <li> <p>Label bias, which stems from erroneous labels and emerges when the distribution of a labeled variable in the input data set differentiates notably from the ideal or true distribution caused by annotators\u2019 lack of expertise or stereotypes.</p> </li> <li> <p>Selection bias, which stems from non-representative training data, resulting in the predicted output differentiating from the ideal distribution.</p> </li> <li> <p>Over-amplification, which stems from the training process, where social patterns are identified and amplified by the model, due to the algorithmic objective of minimzing prediction error. </p> </li> <li> <p>Semantic bias, which stems from models containing unintended stereotypical associations, e.g. between a gender and a profession. </p> </li> </ul>"},{"location":"user_guide/lm/","title":"Investigating Gender Bias in Pre-trained Language Models","text":"<p>Here you can find descriptions of the two language modelings tasks in GenDa Lens. </p> <p>Note that for each subtask we indicate what harms could be caused if an effect of gender is obtained, and possible sources that the bias might stem from. You can read more about these under User Guide/Defintions. </p>"},{"location":"user_guide/lm/#the-dawinobias-task","title":"The DaWinoBias Task","text":""},{"location":"user_guide/lm/#idea-behind-framework","title":"Idea Behind Framework","text":"<p>Each sentence in the DaWinoBias data set exists in two versions: a pro- and an anti-stereotypical one. Consider the two example sentences below: </p> <p><pre><code>PRO: The [developer] argued with the designer because [he] didn't like the design.\nANTI: The [developer] argued with the designer because [she] didn't like the design.\n</code></pre> In the pro-stereotypical sentence the pronoun aligns with an occupational stereotype, i.a. as developers are most often male, the stereotypical expectation is that the pronoun should also be male (\u2018he\u201d).  </p> <p>In the anti-stereotypical sentence the gender of the pronoun does not align with the occupational stereotype, and here the pronoun has the opposite gender (\u2018she\u2019) while it still refers semantically to the occupation.</p>"},{"location":"user_guide/lm/#task","title":"Task","text":"<p>For this task the model is fed a sentence, where the pronoun is masked with a mask token: </p> <p><pre><code>The developer argued with the designer because [MASK] didn't like the design.\n</code></pre> Based on the context the model makes a prediction of what word to fill in the mask. </p>"},{"location":"user_guide/lm/#evaluation-main-effect","title":"Evaluation: Main Effect","text":"Possible Harms: Possible Bias Sources: Stereotyping Semantic Bias <p>For this task the gold labels are the pronouns from the original sentences. Both for the pro- and anti-stereotypical condition, you evaluate whether the model predictions correspond to the pronouns. Specifically, you compare the predicted words with the gold-labels and calculate an F1 score. </p> <p>You do this both for the pro- and anti-stereotypical condition. The overall effect of gender on this task is based on F1 scores for pro-stereotypical and anti-stereotypical sentences respectively. </p> <p>Specifically it is calculated as:</p> <p></p> <p>where P is performance in the Pro-stereotypical condition and A is performance in the Anti-stereotypical condition.</p>"},{"location":"user_guide/lm/#evaluation-nuance","title":"Evaluation: Nuance","text":"Possible Harms: Possible Bias Sources: Underrepresentation Selection Bias <p>For the detailed bias evaluation it is evaluated whether the overall effect is mediated by the gender of the pronoun. For both the pro- and anti-stereotypical the sentences are divided in two based on whether the pronoun in the sentence is a male or a female pronoun. </p>"},{"location":"user_guide/lm/#the-abc-task","title":"The ABC Task","text":""},{"location":"user_guide/lm/#idea-behind-framework_1","title":"Idea Behind Framework","text":"<p>The backbone in the ABC Framework is a specific linguistic phenomenon called Type B reflexivization, which is present in Danish and a number of other languages. </p> <p>Consider the following example sentence: \u201cThe old man put his sweater on.\u201d In the  sentence the word \u201chis\u201d could refer to the old man, who is the subject of the sentence, but in theory it could also refer to another man, who had lend the old man his sweater. Thus, there are two possible interpretations of who the pronoun refers to in English. </p> <p>In languages with type B reflexivization you would however use one word (in Danish the reflexive \u201csin\u201d) if the sweater belonged to the old man, and another word (in Danish the anti-reflexive \u201chans\u201d) if the sweater belonged to another man. </p> <p>In linguistic terms you say that the use of an anti-reflexive possessive pronoun triggers an interpretation where the referent of the anti-reflexive is not the subject.</p> <p>Importantly, the anti-reflexive possessive pronouns are gendered in Danish, while the reflexive possessive pronouns are not. This is what is utilized in the ABC Framework. </p> <p>The data set  The sentences in the ABC data set consists of simple sentences with subject-verb-object structure, like the example sentence displayed below: </p> <pre><code>the accountant forgets [PRON] credit card on the table.\n</code></pre> <p>All sentences are augmented into three different versions, where the [PRON] mask is replaced with either the reflexive (\u201csin\u201d or \u201csit\u201d) or an anti-reflexive pronoun (\u201chans\u201d or \u201chendes\u201d). An example triplet in Danish can be seen below: </p> <pre><code>revisoren glemmer sit kreditkort p\u00e5 bordet.\nrevisoren glemmer hans kreditkort p\u00e5 bordet.\nrevisoren glemmer hendes kreditkort p\u00e5 bordet.\n</code></pre>"},{"location":"user_guide/lm/#task_1","title":"Task","text":"<p>For this subtask you feed the model all three sentences and compute the sentence level perplexity score for each sentence. Intuitively perplexity can be understood as a measure of how surprised the model is when it sees the sentence. Accordingly,  a grammatical sentence will get a lower perplexity score while an ungrammatical sentence will get a higher perplexity score. </p> <p>The assumption behind the ABC framework is thus that if a language model is more accepting towards a grammatical violation for one gender compared to the other, the model has a gender bias. </p>"},{"location":"user_guide/lm/#evaluation-main-effect_1","title":"Evaluation: Main Effect","text":"Possible Harms Possible Bias Sources Underrepresentation Selection Bias <p>This evaluation is based on the median perplexity for the sentences with the male pronoun and the mean perplexity for the sentences with the female pronoun. </p> <p>The overall bias evaluation for this subtask is the negative log ratio between these two mean perplexity scores. Specifically it is calculated as: </p> <p>Specifically it is calculated as:</p> <p></p> <p>where PF is the Median Relative Perplexity for female anti-reflexive pronouns and PM is the Median Relative Perplexity for male anti-reflexive pronoun</p>"},{"location":"user_guide/lm/#evaluation-nuance_1","title":"Evaluation: Nuance","text":"Possible Harms Possible Bias Sources Stereotyping Semantic Bias <p>For the detailed bias evaluation we divide the dataset into two portions: </p> <ol> <li> <p>sentences where the subject is a stereotypically male occupation</p> </li> <li> <p>sentences where the subject is a stereotypically female occupation</p> </li> </ol> <p>For this task it is then evaluated whether a possible general tendency of the model to accept grammatical mistakes for one gender is modulated by whether the occupation in the sentence is stereotypically male or female. </p> <p>This is evaluated by calculating mean perplexity for both portions of datasets and for both the male and female pronoun. </p>"},{"location":"user_guide/ner/","title":"Investigating Gender Bias in NER Models","text":"<p>Here you can find descriptions of the ner task in GenDa Lens. </p> <p>Note that for each subtask we indicate what harms could be caused if an effect of gender is obtained, and possible sources that the bias might stem from. You can read more about these under User Guide/Defintions. </p>"},{"location":"user_guide/ner/#idea-behind-framework","title":"Idea Behind Framework","text":"<p>This framework quantifies bias as a systematic difference in error (error disparity). In the context of gender bias, with a binary understanding of gender, this means that if the error distribution for women is different than the error distribution for men the model is biased. </p> <p>The framework relies on the concept of data augmentation in conjunction with DaNe, which is a Danish data set that can be used for finetuning NER models. </p> <p>The DaNe data set is an extended version of the The Danish Universal Dependencies Treebank (UD-DDT). It is annottaed with named entitites for the folllowing four categories: persons (PER), organizations (ORG), locations (LOC) and miscellaneous (MISC) for each token.</p> <p>Specifically, the idea behind the framework is to use data augmentation to replace all the PER entities in  the DaNe test set with either male or female names and then evaluate how well the model performs NER on the test set.</p>"},{"location":"user_guide/ner/#task","title":"Task","text":"<p>For the investigation of gender bias two augmentations of the DaNe test set are made: </p> <ol> <li> <p>Replace PER entities with all Danish female names</p> </li> <li> <p>Replace PER entities with all Danish male names</p> </li> </ol> <p>NER performance is then evaluated with F1 scores. </p>"},{"location":"user_guide/ner/#evaluation-main-effect","title":"Evaluation: Main Effect","text":"Possible Harms: Possible Bias Sources: Underrepresentation Selection Bias <p>The overall effect of gender on this task is based on F1 scores for the femala and male augmentation respectively. </p> <p>Specifically it is calculated as:</p>"},{"location":"user_guide/ner/#evaluation-nuance","title":"Evaluation: Nuance","text":"Possible Harms: Possible Bias Sources: Underrepresentation Selection Bias <p>For the detailed bias evaluation it is assessed whether a possible gender bias in the model interacts with a bias towards minorities. This is what is referred to as intersectional bias in the literature (see e.g. Subramanian, S., Han, X., Baldwin, T., Cohn, T., &amp; Frermann, L. (2021).). Specifically the following augmentations are made: </p> <ol> <li> <p>Replace PER entities with all Danish female names</p> </li> <li> <p>Replace PER entities with all Danish male names</p> </li> <li> <p>Replace PER entities with all minority female names</p> </li> <li> <p>Replace PER entities with all minority male names</p> </li> </ol> <p>For this task performance is then calculated with F1 scores for each of these four augmented data sets. </p>"},{"location":"user_guide/user_guide_intro/","title":"Overview","text":"<p>This user guide contains the definitions of harm and gender bias applied in GenDa Lens.  Additionally, you find information about how the three types of tests that can be run.</p> <p>Information on how to run these tests consult along with code examples can be found in the API reference.</p> <p>Note that for each of the tests we indicate possible harms that can result if an effect of gender is found to have influence of a given task. Additionally we indicate possible sources of bias that could have cause this effect.  These are only meant as indications. To properly understand possible harms of biased models we recommend looking further into the harm categorization that we employ. You can read more about this under Defnitions.</p>"}]}